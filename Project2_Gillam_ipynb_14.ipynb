{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSBJu6bu7PU",
        "outputId": "eee6a436-bb66-4461-fc64-9dc8a18efe6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ.\n",
            "â³ Vocab ìƒì„± ì¤‘...\n",
            "âœ… Vocab í¬ê¸°: 3702\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_train.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_dev.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_test.csv\n",
            "\n",
            "ğŸš€ í•™ìŠµ ì‹œì‘ (Bi-LSTM + MLP + R-Drop + FGM)\n",
            "| Epoch 01 | Loss 0.6120 | Dev Acc 0.7794 (Best!)\n",
            "| Epoch 02 | Loss 0.5033 | Dev Acc 0.7059\n",
            "| Epoch 03 | Loss 0.4192 | Dev Acc 0.7206\n",
            "| Epoch 04 | Loss 0.3481 | Dev Acc 0.8382 (Best!)\n",
            "| Epoch 05 | Loss 0.2620 | Dev Acc 0.7353\n",
            "| Epoch 06 | Loss 0.1667 | Dev Acc 0.7794\n",
            "| Epoch 07 | Loss 0.1084 | Dev Acc 0.8529 (Best!)\n",
            "| Epoch 08 | Loss 0.0769 | Dev Acc 0.7941\n",
            "| Epoch 09 | Loss 0.0987 | Dev Acc 0.8235\n",
            "| Epoch 10 | Loss 0.0607 | Dev Acc 0.8235\n",
            "| Epoch 11 | Loss 0.0502 | Dev Acc 0.8676 (Best!)\n",
            "| Epoch 12 | Loss 0.0142 | Dev Acc 0.8382\n",
            "| Epoch 13 | Loss 0.0152 | Dev Acc 0.8235\n",
            "| Epoch 14 | Loss 0.0336 | Dev Acc 0.8676\n",
            "| Epoch 15 | Loss 0.0378 | Dev Acc 0.8824 (Best!)\n",
            "| Epoch 16 | Loss 0.0225 | Dev Acc 0.8382\n",
            "| Epoch 17 | Loss 0.0692 | Dev Acc 0.8971 (Best!)\n",
            "| Epoch 18 | Loss 0.0193 | Dev Acc 0.8235\n",
            "| Epoch 19 | Loss 0.0099 | Dev Acc 0.8529\n",
            "| Epoch 20 | Loss 0.0160 | Dev Acc 0.8971\n",
            "| Epoch 21 | Loss 0.0423 | Dev Acc 0.8676\n",
            "| Epoch 22 | Loss 0.0303 | Dev Acc 0.8529\n",
            "| Epoch 23 | Loss 0.0355 | Dev Acc 0.8235\n",
            "| Epoch 24 | Loss 0.0262 | Dev Acc 0.8235\n",
            "| Epoch 25 | Loss 0.0377 | Dev Acc 0.8235\n",
            "| Epoch 26 | Loss 0.0188 | Dev Acc 0.8529\n",
            "| Epoch 27 | Loss 0.0138 | Dev Acc 0.8235\n",
            "| Epoch 28 | Loss 0.0133 | Dev Acc 0.8235\n",
            "| Epoch 29 | Loss 0.0042 | Dev Acc 0.8088\n",
            "| Epoch 30 | Loss 0.0033 | Dev Acc 0.8088\n",
            "\n",
            "ğŸ”„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "ğŸ† Final Test Accuracy: 80.88%\n",
            "\n",
            "ğŸ“Š Dev Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\n",
            "\n",
            "ğŸ“Š Test Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\n",
            "\n",
            "ì´ ì˜¤ë‹µ ê°œìˆ˜: 22 (Dev: 9, Test: 13)\n",
            "\n",
            "[ì˜¤ë‹µ í†µê³„]\n",
            "Gender\n",
            "m    15\n",
            "f     7\n",
            "Name: count, dtype: int64\n",
            "Age\n",
            "5     2\n",
            "6     2\n",
            "7     2\n",
            "8     7\n",
            "9     6\n",
            "10    3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ==========================================\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
        "PROJECT_PATH = '/content/drive/MyDrive/DL_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "try:\n",
        "    from common.np import *\n",
        "    from common.time_layers import TimeEmbedding, TimeLSTM, TimeDropout\n",
        "    from common.layers import Affine, SoftmaxWithLoss, Sigmoid, Dropout\n",
        "    from common.optimizer import Adam\n",
        "    from utils import extract_utterances\n",
        "    print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ.\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ì‹œê°í™” ì„¤ì •\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# ==========================================\n",
        "# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”\n",
        "# ==========================================\n",
        "def load_vocab(train_csv, max_vocab=4000):\n",
        "    df = pd.read_csv(train_csv)\n",
        "    words = []\n",
        "    print(\"â³ Vocab ìƒì„± ì¤‘...\")\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            words.extend(text.split())\n",
        "        except: continue\n",
        "    counter = Counter(words)\n",
        "    most_common = counter.most_common(max_vocab)\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for i, (w, _) in enumerate(most_common):\n",
        "        vocab[w] = i + 2\n",
        "    print(f\"âœ… Vocab í¬ê¸°: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "def load_data(csv_path, vocab, max_len=400):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    x_data, t_data = [], []\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}\")\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            # ê¸¸ì´ ì œí•œ (Truncation)\n",
        "            if len(ids) > max_len:\n",
        "                ids = ids[:max_len]\n",
        "\n",
        "            # íŒ¨ë”© (Padding)\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_data.append(padded_ids)\n",
        "            t_data.append(label_map[row['group']])\n",
        "        except: pass\n",
        "    return np.array(x_data), np.array(t_data)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ì •ì˜ (Bi-LSTM + MLP)\n",
        "# ==========================================\n",
        "class TimeBiLSTM:\n",
        "    def __init__(self, Wx1, Wh1, b1, Wx2, Wh2, b2, stateful=False):\n",
        "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
        "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
        "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
        "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
        "\n",
        "    def forward(self, xs):\n",
        "        o1 = self.forward_lstm.forward(xs)\n",
        "        xs_reverse = xs[:, ::-1, :]\n",
        "        o2 = self.backward_lstm.forward(xs_reverse)\n",
        "        o2 = o2[:, ::-1, :]\n",
        "        return np.concatenate((o1, o2), axis=2)\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        H = dhs.shape[2] // 2\n",
        "        do1 = dhs[:, :, :H]\n",
        "        do2 = dhs[:, :, H:]\n",
        "        dxs1 = self.forward_lstm.backward(do1)\n",
        "        do2_reverse = do2[:, ::-1, :]\n",
        "        dxs2 = self.backward_lstm.backward(do2_reverse)\n",
        "        dxs2 = dxs2[:, ::-1, :]\n",
        "        return dxs1 + dxs2\n",
        "\n",
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "        hr = h.reshape(N, 1, H)\n",
        "        t = np.sum(hs * hr, axis=2)\n",
        "        t -= np.max(t, axis=1, keepdims=True)\n",
        "        exp_t = np.exp(t)\n",
        "        a = exp_t / np.sum(exp_t, axis=1, keepdims=True)\n",
        "        ar = a.reshape(N, T, 1)\n",
        "        c = np.sum(hs * ar, axis=1)\n",
        "        self.cache = (hs, hr, a)\n",
        "        return c\n",
        "\n",
        "    def backward(self, dc):\n",
        "        hs, hr, a = self.cache\n",
        "        N, T, H = hs.shape\n",
        "        ar = a.reshape(N, T, 1)\n",
        "        d_hs = dc.reshape(N, 1, H) * ar\n",
        "        da = np.sum(dc.reshape(N, 1, H) * hs, axis=2)\n",
        "        dt = a * (da - np.sum(da * a, axis=1, keepdims=True))\n",
        "        d_hs += dt.reshape(N, T, 1) * hr\n",
        "        d_hr = np.sum(dt.reshape(N, T, 1) * hs, axis=1)\n",
        "        dh = d_hr.reshape(N, H)\n",
        "        return d_hs, dh\n",
        "\n",
        "class BiAttentionMLPClassifier:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size, class_size=2, dropout_ratio=0.5):\n",
        "        V, D, H, C = vocab_size, wordvec_size, hidden_size, class_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 1. Embedding & Bi-LSTM\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "        lstm_Wx2 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        # 2. MLP Head (4H -> H -> C)\n",
        "        input_size = 4 * H\n",
        "        W1 = (rn(input_size, H) / np.sqrt(input_size)).astype('f')\n",
        "        b1 = np.zeros(H).astype('f')\n",
        "        W2 = (rn(H, C) / np.sqrt(H)).astype('f')\n",
        "        b2 = np.zeros(C).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeBiLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, lstm_Wx2, lstm_Wh2, lstm_b2)\n",
        "        self.attention = Attention()\n",
        "        self.dropout1 = TimeDropout(dropout_ratio)\n",
        "        self.affine1 = Affine(W1, b1)\n",
        "        self.activation = Sigmoid()\n",
        "        self.dropout2 = Dropout(dropout_ratio)\n",
        "        self.affine2 = Affine(W2, b2)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine1, self.affine2):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        hs = self.dropout1.forward(hs)\n",
        "        h_last = hs[:, -1, :]\n",
        "        c = self.attention.forward(hs, h_last)\n",
        "        out = np.concatenate((c, h_last), axis=1)\n",
        "\n",
        "        out = self.affine1.forward(out)\n",
        "        out = self.activation.forward(out)\n",
        "        out = self.dropout2.forward(out)\n",
        "        score = self.affine2.forward(out)\n",
        "        return score\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        dout = self.affine2.backward(dout)\n",
        "        dout = self.dropout2.backward(dout)\n",
        "        dout = self.activation.backward(dout)\n",
        "        dout = self.affine1.backward(dout)\n",
        "\n",
        "        N, H4 = dout.shape\n",
        "        H2 = H4 // 2\n",
        "        dc, dh_last = dout[:, :H2], dout[:, H2:]\n",
        "        d_hs_att, dh_att = self.attention.backward(dc)\n",
        "        dhs = d_hs_att\n",
        "        dhs[:, -1, :] += (dh_last + dh_att)\n",
        "\n",
        "        dhs = self.dropout1.backward(dhs)\n",
        "        dhs = self.lstm.backward(dhs)\n",
        "        self.embed.backward(dhs)\n",
        "        return None\n",
        "\n",
        "    def set_train_flg(self, train_flg=True):\n",
        "        self.dropout1.train_flg = train_flg\n",
        "        self.dropout2.train_flg = train_flg\n",
        "\n",
        "# ==========================================\n",
        "# 4. FGM (ì ëŒ€ì  í•™ìŠµ) êµ¬í˜„ - Numpy ë²„ì „\n",
        "# ==========================================\n",
        "class FGM:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.backup = {}\n",
        "\n",
        "    def attack(self, epsilon=0.05): # Epsilonì„ ì‘ê²Œ ì„¤ì • (ì¤‘ìš”)\n",
        "        # ì„ë² ë”© ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜(W)ì— ë…¸ì´ì¦ˆ ì¶”ê°€\n",
        "        # model.embed.params[0]ì€ ì„ë² ë”© í–‰ë ¬\n",
        "        W = self.model.embed.params[0]\n",
        "        grad = self.model.embed.grads[0]\n",
        "\n",
        "        # ë°±ì—…\n",
        "        self.backup['embed_W'] = W.copy()\n",
        "\n",
        "        # ë…¸ì´ì¦ˆ ê³„ì‚° (Gradient ë°©í–¥)\n",
        "        norm = np.sqrt(np.sum(grad**2))\n",
        "        if norm != 0:\n",
        "            r_at = epsilon * grad / norm\n",
        "            W += r_at # ê³µê²© ì ìš©\n",
        "\n",
        "    def restore(self):\n",
        "        # ì›ìƒ ë³µêµ¬\n",
        "        if 'embed_W' in self.backup:\n",
        "            self.model.embed.params[0][...] = self.backup['embed_W']\n",
        "        self.backup = {}\n",
        "\n",
        "# ==========================================\n",
        "# 5. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰\n",
        "# ==========================================\n",
        "MAX_LEN = 300       # ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ ì ì ˆí•œ ê¸¸ì´ ìœ ì§€\n",
        "BATCH_SIZE = 16\n",
        "MAX_EPOCH = 30      # FGM ë•ë¶„ì— ê³¼ì í•©ì´ ëœí•˜ë¯€ë¡œ ì—í­ì„ ëŠ˜ë¦¼\n",
        "HIDDEN_SIZE = 128\n",
        "WORDVEC_SIZE = 100\n",
        "DROPOUT_RATIO = 0.5\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "train_csv = 'split/gillam_train.csv'\n",
        "dev_csv = 'split/gillam_dev.csv'\n",
        "test_csv = 'split/gillam_test.csv'\n",
        "\n",
        "vocab = load_vocab(train_csv, max_vocab=4000)\n",
        "x_train, t_train = load_data(train_csv, vocab, max_len=MAX_LEN)\n",
        "x_dev, t_dev = load_data(dev_csv, vocab, max_len=MAX_LEN)\n",
        "x_test, t_test = load_data(test_csv, vocab, max_len=MAX_LEN)\n",
        "\n",
        "if np.__name__ == 'cupy':\n",
        "    x_train, t_train = np.asarray(x_train), np.asarray(t_train)\n",
        "    x_dev, t_dev = np.asarray(x_dev), np.asarray(t_dev)\n",
        "    x_test, t_test = np.asarray(x_test), np.asarray(t_test)\n",
        "\n",
        "# ëª¨ë¸ & FGM ì´ˆê¸°í™”\n",
        "model = BiAttentionMLPClassifier(len(vocab), WORDVEC_SIZE, HIDDEN_SIZE, dropout_ratio=DROPOUT_RATIO)\n",
        "optimizer = Adam(lr=LEARNING_RATE)\n",
        "fgm = FGM(model)\n",
        "\n",
        "data_size = len(x_train)\n",
        "max_iters = data_size // BATCH_SIZE\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ (Bi-LSTM + MLP + R-Drop + FGM)\")\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model.set_train_flg(True)\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x_train = x_train[idx]\n",
        "    t_train = t_train[idx]\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        batch_x = x_train[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
        "        batch_t = t_train[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
        "\n",
        "        # 1. ì¼ë°˜ í•™ìŠµ (Clean Data)\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward() # ê¸°ìš¸ê¸° ê³„ì‚°\n",
        "\n",
        "        # 2. ì ëŒ€ì  í•™ìŠµ (Adversarial Data - FGM)\n",
        "        fgm.attack(epsilon=0.05) # ê³µê²©! (ì„ë² ë”©ì— ë…¸ì´ì¦ˆ)\n",
        "\n",
        "        loss_adv = model.forward(batch_x, batch_t) # ë…¸ì´ì¦ˆ ë‚€ ìƒíƒœë¡œ Loss ê³„ì‚°\n",
        "        model.backward() # ê¸°ìš¸ê¸° ëˆ„ì  (Gradient Accumulation íš¨ê³¼)\n",
        "\n",
        "        fgm.restore() # ë³µêµ¬\n",
        "\n",
        "        # 3. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (Clean + Adv ê¸°ìš¸ê¸° ëª¨ë‘ ë°˜ì˜)\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # í‰ê°€\n",
        "    model.set_train_flg(False)\n",
        "    model_score = model.predict(x_dev)\n",
        "    pred = np.argmax(model_score, axis=1)\n",
        "    acc = np.sum(pred == t_dev) / len(t_dev)\n",
        "\n",
        "    avg_loss = total_loss / loss_count\n",
        "\n",
        "    # ìµœê³  ëª¨ë¸ ì €ì¥\n",
        "    if acc > best_valid_acc:\n",
        "        best_valid_acc = acc\n",
        "        with open('best_model.pkl', 'wb') as f:\n",
        "            pickle.dump(model.params, f)\n",
        "        print(f\"| Epoch {epoch+1:02} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f} (Best!)\")\n",
        "    else:\n",
        "        print(f\"| Epoch {epoch+1:02} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. ìµœì¢… í‰ê°€ & ì˜¤ë‹µ ë¶„ì„ (Length Fixed)\n",
        "# ==========================================\n",
        "print(\"\\nğŸ”„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "with open('best_model.pkl', 'rb') as f:\n",
        "    best_params = pickle.load(f)\n",
        "for i, param in enumerate(best_params):\n",
        "    model.params[i][...] = param\n",
        "\n",
        "# Test\n",
        "model.set_train_flg(False)\n",
        "test_score = model.predict(x_test)\n",
        "test_pred = np.argmax(test_score, axis=1)\n",
        "test_acc = np.sum(test_pred == t_test) / len(t_test)\n",
        "print(f\"ğŸ† Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "# ì˜¤ë‹µ ë¶„ì„ (ê¸¸ì´ ì œí•œ ì ìš©)\n",
        "def analyze_errors_fixed(model, df, vocab, dataset_name=\"Dev\", max_len=300):\n",
        "    error_records = []\n",
        "    model.set_train_flg(False)\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "    inv_label_map = {0: 'TD', 1: 'SLI'}\n",
        "\n",
        "    print(f\"\\nğŸ“Š {dataset_name} Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            # [ì¤‘ìš”] Truncation ì ìš©\n",
        "            if len(ids) > max_len: ids = ids[:max_len]\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_in = padded_ids.reshape(1, -1)\n",
        "            if np.__name__ == 'cupy': x_in = np.asarray(x_in)\n",
        "\n",
        "            score = model.predict(x_in)\n",
        "            if np.__name__ == 'cupy': score = np.asnumpy(score)\n",
        "\n",
        "            pred_idx = np.argmax(score, axis=1)[0]\n",
        "            true_idx = label_map[row['group']]\n",
        "\n",
        "            if pred_idx != true_idx:\n",
        "                error_records.append({\n",
        "                    'Index': idx,\n",
        "                    'Gender': row['gender'],\n",
        "                    'Age': row['age'],\n",
        "                    'True_Label': row['group'],\n",
        "                    'Pred_Label': inv_label_map[pred_idx],\n",
        "                    'Word_Count': len(tokens)\n",
        "                })\n",
        "        except: continue\n",
        "    return pd.DataFrame(error_records)\n",
        "\n",
        "dev_errors = analyze_errors_fixed(model, pd.read_csv('split/gillam_dev.csv'), vocab, \"Dev\", MAX_LEN)\n",
        "test_errors = analyze_errors_fixed(model, pd.read_csv('split/gillam_test.csv'), vocab, \"Test\", MAX_LEN)\n",
        "all_errors = pd.concat([dev_errors, test_errors])\n",
        "\n",
        "print(f\"\\nì´ ì˜¤ë‹µ ê°œìˆ˜: {len(all_errors)} (Dev: {len(dev_errors)}, Test: {len(test_errors)})\")\n",
        "if not all_errors.empty:\n",
        "    print(\"\\n[ì˜¤ë‹µ í†µê³„]\")\n",
        "    print(all_errors['Gender'].value_counts())\n",
        "    print(all_errors['Age'].value_counts().sort_index())"
      ]
    }
  ]
}