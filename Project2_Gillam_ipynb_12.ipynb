{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSBJu6bu7PU",
        "outputId": "b0c16cdd-9d8a-4426-8769-9aa207413c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ: numpy\n",
            "â³ Vocab ìƒì„± ì¤‘...\n",
            "âœ… Vocab í¬ê¸°: 3702\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_train.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_dev.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_test.csv\n",
            "\n",
            "ğŸš€ í•™ìŠµ ì‹œì‘ (Bi-LSTM + MLP + R-Drop + FGM)\n",
            "| Epoch 01 | Loss 0.7168 | Dev Acc 0.7353 (Best!)\n",
            "| Epoch 02 | Loss 0.5989 | Dev Acc 0.7353\n",
            "| Epoch 03 | Loss 0.5958 | Dev Acc 0.7353\n",
            "| Epoch 04 | Loss 0.5927 | Dev Acc 0.7353\n",
            "| Epoch 05 | Loss 0.5866 | Dev Acc 0.7353\n",
            "| Epoch 06 | Loss 0.5837 | Dev Acc 0.7353\n",
            "| Epoch 07 | Loss 0.5892 | Dev Acc 0.7353\n",
            "| Epoch 08 | Loss 0.5840 | Dev Acc 0.7353\n",
            "| Epoch 09 | Loss 0.5783 | Dev Acc 0.7353\n",
            "| Epoch 10 | Loss 0.5688 | Dev Acc 0.7353\n",
            "| Epoch 11 | Loss 0.5483 | Dev Acc 0.7353\n",
            "| Epoch 12 | Loss 0.5453 | Dev Acc 0.7353\n",
            "| Epoch 13 | Loss 0.5379 | Dev Acc 0.7353\n",
            "| Epoch 14 | Loss 0.4984 | Dev Acc 0.7353\n",
            "| Epoch 15 | Loss 0.4625 | Dev Acc 0.7353\n",
            "| Epoch 16 | Loss 0.4249 | Dev Acc 0.7500 (Best!)\n",
            "| Epoch 17 | Loss 0.3827 | Dev Acc 0.7647 (Best!)\n",
            "| Epoch 18 | Loss 0.3282 | Dev Acc 0.7794 (Best!)\n",
            "| Epoch 19 | Loss 0.2803 | Dev Acc 0.7941 (Best!)\n",
            "| Epoch 20 | Loss 0.2581 | Dev Acc 0.8088 (Best!)\n",
            "| Epoch 21 | Loss 0.2012 | Dev Acc 0.7941\n",
            "| Epoch 22 | Loss 0.1777 | Dev Acc 0.7941\n",
            "| Epoch 23 | Loss 0.1890 | Dev Acc 0.7941\n",
            "| Epoch 24 | Loss 0.1699 | Dev Acc 0.7941\n",
            "| Epoch 25 | Loss 0.1197 | Dev Acc 0.7794\n",
            "| Epoch 26 | Loss 0.1088 | Dev Acc 0.7647\n",
            "| Epoch 27 | Loss 0.1101 | Dev Acc 0.8235 (Best!)\n",
            "| Epoch 28 | Loss 0.1031 | Dev Acc 0.7941\n",
            "| Epoch 29 | Loss 0.1085 | Dev Acc 0.7647\n",
            "| Epoch 30 | Loss 0.0889 | Dev Acc 0.7941\n",
            "\n",
            "ğŸ”„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "ğŸ† Final Test Accuracy: 82.35%\n",
            "\n",
            "ğŸ“Š Dev Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\n",
            "\n",
            "ğŸ“Š Test Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\n",
            "\n",
            "ì´ ì˜¤ë‹µ ê°œìˆ˜: 26 (Dev: 12, Test: 14)\n",
            "\n",
            "[ì˜¤ë‹µ í†µê³„]\n",
            "Gender\n",
            "m    19\n",
            "f     7\n",
            "Name: count, dtype: int64\n",
            "Age\n",
            "5     5\n",
            "6     3\n",
            "7     4\n",
            "8     7\n",
            "9     4\n",
            "10    3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ==========================================\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
        "PROJECT_PATH = '/content/drive/MyDrive/DL_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "try:\n",
        "    from common.np import *\n",
        "    from common.time_layers import TimeEmbedding, TimeLSTM, TimeDropout\n",
        "    from common.layers import Affine, SoftmaxWithLoss, Sigmoid, Dropout\n",
        "    from common.optimizer import Adam\n",
        "    from utils import extract_utterances\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from collections import Counter\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "    print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ:\", np.__name__)\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# ==========================================\n",
        "# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (R-Drop, FGM)\n",
        "# ==========================================\n",
        "\n",
        "# 1) R-Dropìš© KL-Divergence ì†ì‹¤ í•¨ìˆ˜ (Numpy/CuPy ë²„ì „)\n",
        "def compute_kl_loss_numpy(p, q):\n",
        "    # p, q: Softmax í™•ë¥ ê°’ (N, C)\n",
        "    # KL(p||q) = sum(p * log(p/q))\n",
        "    # ì–‘ë°©í–¥: 0.5 * (KL(p||q) + KL(q||p))\n",
        "\n",
        "    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’ ë”í•¨\n",
        "    epsilon = 1e-8\n",
        "    p = p + epsilon\n",
        "    q = q + epsilon\n",
        "\n",
        "    kl_pq = np.sum(p * (np.log(p) - np.log(q)), axis=1)\n",
        "    kl_qp = np.sum(q * (np.log(q) - np.log(p)), axis=1)\n",
        "\n",
        "    loss = (np.mean(kl_pq) + np.mean(kl_qp)) / 2\n",
        "    return loss\n",
        "\n",
        "# 2) FGM í´ë˜ìŠ¤ (Numpy/CuPy ë²„ì „ - ì„ë² ë”© ì¸µ ê³µê²©)\n",
        "class FGM_Numpy:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.backup = {}\n",
        "\n",
        "    def attack(self, epsilon=1.0, emb_layer_name='embed'):\n",
        "        # ì„ë² ë”© ë ˆì´ì–´ ì°¾ê¸°\n",
        "        emb_layer = getattr(self.model, emb_layer_name, None)\n",
        "        if emb_layer is None: return\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜(W)ê°€ ì•„ë‹Œ, forward ì‹œì˜ ì…ë ¥ì— ëŒ€í•œ ê¸°ìš¸ê¸°ê°€ í•„ìš”í•˜ë¯€ë¡œ\n",
        "        # ì—¬ê¸°ì„œëŠ” ì„ë² ë”© ê°€ì¤‘ì¹˜(W) ìì²´ì— ë…¸ì´ì¦ˆë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ (Simpler FGM)\n",
        "        # ë˜ëŠ” Embedding Layerì˜ backwardì—ì„œ dWë¥¼ ì´ìš©\n",
        "\n",
        "        W = emb_layer.params[0] # Embedding Weight\n",
        "        dW = emb_layer.grads[0] # Embedding Gradient\n",
        "\n",
        "        self.backup['W'] = W.copy()\n",
        "\n",
        "        # Norm ê³„ì‚°\n",
        "        norm = np.sqrt(np.sum(dW**2))\n",
        "        if norm != 0:\n",
        "            r_at = epsilon * dW / norm\n",
        "            W += r_at # ê³µê²© (Weightì— ë…¸ì´ì¦ˆ ì¶”ê°€)\n",
        "\n",
        "    def restore(self, emb_layer_name='embed'):\n",
        "        emb_layer = getattr(self.model, emb_layer_name, None)\n",
        "        if emb_layer is None: return\n",
        "\n",
        "        if 'W' in self.backup:\n",
        "            emb_layer.params[0][...] = self.backup['W'] # ì›ìƒë³µêµ¬\n",
        "        self.backup = {}\n",
        "\n",
        "# ==========================================\n",
        "# 3. ë°ì´í„° ë¡œë”\n",
        "# ==========================================\n",
        "def load_vocab(train_csv, max_vocab=4000):\n",
        "    df = pd.read_csv(train_csv)\n",
        "    words = []\n",
        "    print(\"â³ Vocab ìƒì„± ì¤‘...\")\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            words.extend(text.split())\n",
        "        except: continue\n",
        "    counter = Counter(words)\n",
        "    most_common = counter.most_common(max_vocab)\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for i, (w, _) in enumerate(most_common):\n",
        "        vocab[w] = i + 2\n",
        "    print(f\"âœ… Vocab í¬ê¸°: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "def load_data(csv_path, vocab, max_len=400):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    x_data, t_data = [], []\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}\")\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            if len(ids) > max_len: ids = ids[:max_len]\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_data.append(padded_ids)\n",
        "            t_data.append(label_map[row['group']])\n",
        "        except: pass\n",
        "    return np.array(x_data), np.array(t_data)\n",
        "\n",
        "# ==========================================\n",
        "# 4. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
        "# ==========================================\n",
        "class TimeBiLSTM:\n",
        "    def __init__(self, Wx1, Wh1, b1, Wx2, Wh2, b2, stateful=False):\n",
        "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
        "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
        "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
        "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
        "\n",
        "    def forward(self, xs):\n",
        "        o1 = self.forward_lstm.forward(xs)\n",
        "        xs_reverse = xs[:, ::-1, :]\n",
        "        o2 = self.backward_lstm.forward(xs_reverse)\n",
        "        o2 = o2[:, ::-1, :]\n",
        "        out = np.concatenate((o1, o2), axis=2)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        H = dhs.shape[2] // 2\n",
        "        do1 = dhs[:, :, :H]\n",
        "        do2 = dhs[:, :, H:]\n",
        "        dxs1 = self.forward_lstm.backward(do1)\n",
        "        do2_reverse = do2[:, ::-1, :]\n",
        "        dxs2 = self.backward_lstm.backward(do2_reverse)\n",
        "        dxs2 = dxs2[:, ::-1, :]\n",
        "        return dxs1 + dxs2\n",
        "\n",
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "        hr = h.reshape(N, 1, H)\n",
        "        t = np.sum(hs * hr, axis=2)\n",
        "        t -= np.max(t, axis=1, keepdims=True)\n",
        "        exp_t = np.exp(t)\n",
        "        a = exp_t / np.sum(exp_t, axis=1, keepdims=True)\n",
        "        ar = a.reshape(N, T, 1)\n",
        "        c = np.sum(hs * ar, axis=1)\n",
        "        self.cache = (hs, hr, a)\n",
        "        return c\n",
        "\n",
        "    def backward(self, dc):\n",
        "        hs, hr, a = self.cache\n",
        "        N, T, H = hs.shape\n",
        "        ar = a.reshape(N, T, 1)\n",
        "        d_hs = dc.reshape(N, 1, H) * ar\n",
        "        da = np.sum(dc.reshape(N, 1, H) * hs, axis=2)\n",
        "        dt = a * (da - np.sum(da * a, axis=1, keepdims=True))\n",
        "        d_hs += dt.reshape(N, T, 1) * hr\n",
        "        d_hr = np.sum(dt.reshape(N, T, 1) * hs, axis=1)\n",
        "        dh = d_hr.reshape(N, H)\n",
        "        return d_hs, dh\n",
        "\n",
        "# Custom Softmax for Probability Output (for R-Drop)\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x - x.max(axis=1, keepdims=True)\n",
        "            y = np.exp(x)\n",
        "            y /= y.sum(axis=1, keepdims=True)\n",
        "        elif x.ndim == 1:\n",
        "            x = x - np.max(x)\n",
        "            y = np.exp(x) / np.sum(np.exp(x))\n",
        "        return y\n",
        "\n",
        "class BiAttentionMLPClassifier:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size, class_size=2, dropout_ratio=0.5):\n",
        "        V, D, H, C = vocab_size, wordvec_size, hidden_size, class_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # Init Weights\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "        lstm_Wx2 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        # MLP Head (4H -> H -> C)\n",
        "        input_size = 4 * H\n",
        "        W1 = (rn(input_size, H) / np.sqrt(input_size)).astype('f')\n",
        "        b1 = np.zeros(H).astype('f')\n",
        "        W2 = (rn(H, C) / np.sqrt(H)).astype('f')\n",
        "        b2 = np.zeros(C).astype('f')\n",
        "\n",
        "        # Layers\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeBiLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, lstm_Wx2, lstm_Wh2, lstm_b2)\n",
        "        self.attention = Attention()\n",
        "        self.dropout1 = TimeDropout(dropout_ratio)\n",
        "        self.affine1 = Affine(W1, b1)\n",
        "        self.activation = Sigmoid()\n",
        "        self.dropout2 = Dropout(dropout_ratio)\n",
        "        self.affine2 = Affine(W2, b2)\n",
        "\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "        self.softmax = Softmax() # For R-Drop probs\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine1, self.affine2):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs, return_prob=False):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        hs = self.dropout1.forward(hs)\n",
        "        h_last = hs[:, -1, :]\n",
        "        c = self.attention.forward(hs, h_last)\n",
        "        out = np.concatenate((c, h_last), axis=1)\n",
        "\n",
        "        out = self.affine1.forward(out)\n",
        "        out = self.activation.forward(out)\n",
        "        out = self.dropout2.forward(out)\n",
        "        score = self.affine2.forward(out)\n",
        "\n",
        "        if return_prob:\n",
        "            return self.softmax.forward(score)\n",
        "        return score\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        dout = self.affine2.backward(dout)\n",
        "        dout = self.dropout2.backward(dout)\n",
        "        dout = self.activation.backward(dout)\n",
        "        dout = self.affine1.backward(dout)\n",
        "\n",
        "        N, H4 = dout.shape\n",
        "        H2 = H4 // 2\n",
        "        dc, dh_last = dout[:, :H2], dout[:, H2:]\n",
        "\n",
        "        d_hs_att, dh_att = self.attention.backward(dc)\n",
        "        dhs = d_hs_att\n",
        "        dhs[:, -1, :] += (dh_last + dh_att)\n",
        "\n",
        "        dhs = self.dropout1.backward(dhs)\n",
        "        dhs = self.lstm.backward(dhs)\n",
        "        self.embed.backward(dhs)\n",
        "        return None\n",
        "\n",
        "    def set_train_flg(self, train_flg=True):\n",
        "        self.dropout1.train_flg = train_flg\n",
        "        self.dropout2.train_flg = train_flg\n",
        "\n",
        "# ==========================================\n",
        "# 5. í•™ìŠµ ì„¤ì •\n",
        "# ==========================================\n",
        "MAX_LEN = 300\n",
        "BATCH_SIZE = 16\n",
        "MAX_EPOCH = 30\n",
        "HIDDEN_SIZE = 128\n",
        "WORDVEC_SIZE = 100\n",
        "DROPOUT_RATIO = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "RDROP_ALPHA = 4.0 # R-Drop ê°€ì¤‘ì¹˜\n",
        "FGM_EPSILON = 1.0 # FGM ê³µê²© í¬ê¸°\n",
        "\n",
        "train_csv = 'split/gillam_train.csv'\n",
        "dev_csv = 'split/gillam_dev.csv'\n",
        "test_csv = 'split/gillam_test.csv'\n",
        "\n",
        "vocab = load_vocab(train_csv, max_vocab=4000)\n",
        "vocab_size = len(vocab)\n",
        "x_train, t_train = load_data(train_csv, vocab, max_len=MAX_LEN)\n",
        "x_dev, t_dev = load_data(dev_csv, vocab, max_len=MAX_LEN)\n",
        "x_test, t_test = load_data(test_csv, vocab, max_len=MAX_LEN)\n",
        "\n",
        "if np.__name__ == 'cupy':\n",
        "    x_train, t_train = np.asarray(x_train), np.asarray(t_train)\n",
        "    x_dev, t_dev = np.asarray(x_dev), np.asarray(t_dev)\n",
        "    x_test, t_test = np.asarray(x_test), np.asarray(t_test)\n",
        "\n",
        "model = BiAttentionMLPClassifier(vocab_size, WORDVEC_SIZE, HIDDEN_SIZE, dropout_ratio=DROPOUT_RATIO)\n",
        "optimizer = Adam(lr=LEARNING_RATE)\n",
        "fgm = FGM_Numpy(model)\n",
        "\n",
        "data_size = len(x_train)\n",
        "max_iters = data_size // BATCH_SIZE\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ (Bi-LSTM + MLP + R-Drop + FGM)\")\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model.set_train_flg(True)\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x_train = x_train[idx]\n",
        "    t_train = t_train[idx]\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        batch_x = x_train[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
        "        batch_t = t_train[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
        "\n",
        "        # --- 1. R-Drop Forward ---\n",
        "        # Dropoutì´ ì¼œì§„ ìƒíƒœë¡œ ë‘ ë²ˆ Forward\n",
        "        prob1 = model.predict(batch_x, return_prob=True)\n",
        "        prob2 = model.predict(batch_x, return_prob=True)\n",
        "\n",
        "        # Loss 1: Cross Entropy (ë‘ ê²°ê³¼ì˜ í‰ê· )\n",
        "        # SoftmaxWithLossëŠ” logitsë¥¼ ë°›ìœ¼ë¯€ë¡œ ë‹¤ì‹œ ê³„ì‚° í•„ìš”í•˜ì§€ë§Œ,\n",
        "        # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ model.forward(logits) í˜¸ì¶œë¡œ ëŒ€ì²´ (ì•½ê°„ì˜ ë¹„íš¨ìœ¨ ê°ìˆ˜)\n",
        "        loss_ce1 = model.forward(batch_x, batch_t)\n",
        "        # ì—­ì „íŒŒ 1\n",
        "        model.backward()\n",
        "        grads1 = [p.copy() for p in model.grads] # ê¸°ìš¸ê¸° ì €ì¥\n",
        "\n",
        "        # ë‘ ë²ˆì§¸ forwardì— ëŒ€í•œ CE LossëŠ” ìƒëµí•˜ê³  KLë§Œ ì¶”ê°€í•˜ê±°ë‚˜,\n",
        "        # ì •ì„ëŒ€ë¡œë¼ë©´ ë‘ ë²ˆì˜ CE í‰ê·  + KL ì´ì–´ì•¼ í•¨.\n",
        "        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ \"CE(1íšŒ) + KL(prob1, prob2)\" ë¡œ êµ¬í˜„ (ì†ë„ ê³ ë ¤)\n",
        "\n",
        "        # Loss 2: KL Divergence\n",
        "        loss_kl = compute_kl_loss_numpy(prob1, prob2)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = loss_ce1 + RDROP_ALPHA * loss_kl\n",
        "\n",
        "        # KL Lossì— ëŒ€í•œ ì—­ì „íŒŒëŠ” ë³µì¡í•˜ë¯€ë¡œ,\n",
        "        # R-Dropì€ ë³´í†µ PyTorch Autogradì— ì˜ì¡´í•©ë‹ˆë‹¤.\n",
        "        # Numpyë¡œ êµ¬í˜„ ì‹œ KL Gradient ìˆ˜ì‹ ì ìš©ì´ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤.\n",
        "        # ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” **FGMë§Œ ì ìš©**í•˜ê³  R-Dropì€ ì œì™¸í•˜ê±°ë‚˜,\n",
        "        # KL Gradientë¥¼ ê·¼ì‚¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "        # -> ì•ˆì „í•˜ê²Œ **FGMë§Œ ë¨¼ì € ì ìš©**í•˜ëŠ” ì½”ë“œë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. (R-Drop ì œì™¸)\n",
        "\n",
        "        # --- FGM Adversarial Training ---\n",
        "        fgm.attack(epsilon=FGM_EPSILON) # 1. ê³µê²© (ì„ë² ë”©ì— ë…¸ì´ì¦ˆ)\n",
        "\n",
        "        loss_adv = model.forward(batch_x, batch_t) # 2. ë…¸ì´ì¦ˆ ë‚€ ìƒíƒœë¡œ Loss\n",
        "        model.backward() # 3. ê¸°ìš¸ê¸° ëˆ„ì  (Clean + Adv)\n",
        "\n",
        "        fgm.restore() # 4. ë³µêµ¬\n",
        "\n",
        "        # --- Update ---\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss += loss_ce1\n",
        "        loss_count += 1\n",
        "\n",
        "    # í‰ê°€\n",
        "    model.set_train_flg(False)\n",
        "    model_score = model.predict(x_dev)\n",
        "    pred = np.argmax(model_score, axis=1)\n",
        "    acc = np.sum(pred == t_dev) / len(t_dev)\n",
        "\n",
        "    avg_loss = total_loss / loss_count\n",
        "\n",
        "    if acc > best_valid_acc:\n",
        "        best_valid_acc = acc\n",
        "        with open('best_model.pkl', 'wb') as f:\n",
        "            pickle.dump(model.params, f)\n",
        "        print(f\"| Epoch {epoch+1:02} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f} (Best!)\")\n",
        "    else:\n",
        "        print(f\"| Epoch {epoch+1:02} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. ìµœì¢… í‰ê°€ & ì˜¤ë‹µ ë¶„ì„ (Length Fixed)\n",
        "# ==========================================\n",
        "print(\"\\nğŸ”„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "with open('best_model.pkl', 'rb') as f:\n",
        "    best_params = pickle.load(f)\n",
        "for i, param in enumerate(best_params):\n",
        "    model.params[i][...] = param\n",
        "\n",
        "# Test\n",
        "model.set_train_flg(False)\n",
        "test_score = model.predict(x_test)\n",
        "test_pred = np.argmax(test_score, axis=1)\n",
        "test_acc = np.sum(test_pred == t_test) / len(t_test)\n",
        "print(f\"ğŸ† Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "# ì˜¤ë‹µ ë¶„ì„ (with Truncation)\n",
        "def analyze_errors_fixed(model, df, vocab, dataset_name=\"Dev\", max_len=400):\n",
        "    error_records = []\n",
        "    model.set_train_flg(False)\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "    inv_label_map = {0: 'TD', 1: 'SLI'}\n",
        "\n",
        "    print(f\"\\nğŸ“Š {dataset_name} Set ì˜¤ë‹µ ë¶„ì„ ì¤‘...\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            # [ì¤‘ìš”] Truncation ì ìš©\n",
        "            if len(ids) > max_len: ids = ids[:max_len]\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_in = padded_ids.reshape(1, -1)\n",
        "            if np.__name__ == 'cupy': x_in = np.asarray(x_in)\n",
        "\n",
        "            score = model.predict(x_in)\n",
        "            if np.__name__ == 'cupy': score = np.asnumpy(score)\n",
        "\n",
        "            pred_idx = np.argmax(score, axis=1)[0]\n",
        "            true_idx = label_map[row['group']]\n",
        "\n",
        "            if pred_idx != true_idx:\n",
        "                error_records.append({\n",
        "                    'Index': idx,\n",
        "                    'Gender': row['gender'],\n",
        "                    'Age': row['age'],\n",
        "                    'True_Label': row['group'],\n",
        "                    'Pred_Label': inv_label_map[pred_idx],\n",
        "                    'Word_Count': len(tokens)\n",
        "                })\n",
        "        except: continue\n",
        "    return pd.DataFrame(error_records)\n",
        "\n",
        "dev_errors = analyze_errors_fixed(model, pd.read_csv('split/gillam_dev.csv'), vocab, \"Dev\", MAX_LEN)\n",
        "test_errors = analyze_errors_fixed(model, pd.read_csv('split/gillam_test.csv'), vocab, \"Test\", MAX_LEN)\n",
        "all_errors = pd.concat([dev_errors, test_errors])\n",
        "\n",
        "print(f\"\\nì´ ì˜¤ë‹µ ê°œìˆ˜: {len(all_errors)} (Dev: {len(dev_errors)}, Test: {len(test_errors)})\")\n",
        "if not all_errors.empty:\n",
        "    print(\"\\n[ì˜¤ë‹µ í†µê³„]\")\n",
        "    print(all_errors['Gender'].value_counts())\n",
        "    print(all_errors['Age'].value_counts().sort_index())"
      ]
    }
  ]
}