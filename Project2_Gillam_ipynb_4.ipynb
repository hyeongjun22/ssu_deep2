{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSBJu6bu7PU",
        "outputId": "d9a5a398-bbf9-48d5-9402-b2f13d45ba4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ: numpy\n",
            "â³ Vocab ìƒì„± ì¤‘...\n",
            "âœ… Vocab í¬ê¸°: 3702\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_train.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_dev.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_test.csv\n",
            "Train Shape: (540, 300)\n",
            "\n",
            "ğŸš€ í•™ìŠµ ì‹œì‘ (Device: numpy)\n",
            "| Epoch 01 | Loss 0.6145 | Dev Acc 0.7353\n",
            "| Epoch 02 | Loss 0.5442 | Dev Acc 0.7353\n",
            "| Epoch 03 | Loss 0.4928 | Dev Acc 0.7353\n",
            "| Epoch 04 | Loss 0.4333 | Dev Acc 0.7059\n",
            "| Epoch 05 | Loss 0.3652 | Dev Acc 0.6765\n",
            "| Epoch 06 | Loss 0.3401 | Dev Acc 0.7059\n",
            "| Epoch 07 | Loss 0.3030 | Dev Acc 0.7206\n",
            "| Epoch 08 | Loss 0.3094 | Dev Acc 0.7206\n",
            "| Epoch 09 | Loss 0.2774 | Dev Acc 0.7059\n",
            "| Epoch 10 | Loss 0.2732 | Dev Acc 0.7353\n",
            "| Epoch 11 | Loss 0.2747 | Dev Acc 0.7206\n",
            "| Epoch 12 | Loss 0.2558 | Dev Acc 0.7353\n",
            "| Epoch 13 | Loss 0.2486 | Dev Acc 0.7500\n",
            "| Epoch 14 | Loss 0.2468 | Dev Acc 0.7500\n",
            "| Epoch 15 | Loss 0.2630 | Dev Acc 0.7500\n",
            "\n",
            "ğŸ† Final Test Accuracy: 72.06%\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ ì§€ì •\n",
        "# ==========================================\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë³¸ì¸ì˜ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
        "PROJECT_PATH = '/content/drive/MyDrive/DL_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (TimeDropout ì¶”ê°€)\n",
        "try:\n",
        "    from common.np import *\n",
        "    from common.time_layers import TimeEmbedding, TimeLSTM, TimeDropout # TimeDropout ì¶”ê°€\n",
        "    from common.layers import Affine, SoftmaxWithLoss\n",
        "    from common.optimizer import Adam\n",
        "    from utils import extract_utterances\n",
        "    import pandas as pd\n",
        "    from collections import Counter\n",
        "    print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ:\", np.__name__)\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë” (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "# ==========================================\n",
        "def load_vocab(train_csv, max_vocab=4000):\n",
        "    df = pd.read_csv(train_csv)\n",
        "    words = []\n",
        "    print(\"â³ Vocab ìƒì„± ì¤‘...\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            words.extend(text.split())\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    counter = Counter(words)\n",
        "    most_common = counter.most_common(max_vocab)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for i, (w, _) in enumerate(most_common):\n",
        "        vocab[w] = i + 2\n",
        "\n",
        "    print(f\"âœ… Vocab í¬ê¸°: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "def load_data(csv_path, vocab, max_len=300):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    x_data = []\n",
        "    t_data = []\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            if len(ids) > max_len:\n",
        "                ids = ids[:max_len]\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_data.append(padded_ids)\n",
        "            t_data.append(label_map[row['group']])\n",
        "        except:\n",
        "            pass\n",
        "    return np.array(x_data), np.array(t_data)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ì •ì˜ (Dropout ì ìš©)\n",
        "# ==========================================\n",
        "class SLPClassifier:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size, class_size=2, dropout_ratio=0.5):\n",
        "        V, D, H, C = vocab_size, wordvec_size, hidden_size, class_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, C) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(C).astype('f')\n",
        "\n",
        "        # ê³„ì¸µ ìƒì„±: LSTM ë’¤ì— Dropout ì¶”ê°€\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False),\n",
        "            TimeDropout(dropout_ratio) # Dropout ì¶”ê°€\n",
        "        ]\n",
        "        self.affine_layer = Affine(affine_W, affine_b)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # íŒŒë¼ë¯¸í„° ìˆ˜ì§‘\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.params += self.affine_layer.params\n",
        "        self.grads += self.affine_layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "\n",
        "        # xs shape: (N, T, H) (Dropout ì ìš©ëœ ìƒíƒœ)\n",
        "        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ ì¶”ì¶œ\n",
        "        h_last = xs[:, -1, :]\n",
        "        score = self.affine_layer.forward(h_last)\n",
        "        return score\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        dout = self.affine_layer.backward(dout)\n",
        "\n",
        "        N, H = dout.shape\n",
        "        # TimeEmbedding(layer[0])ì„ í†µí•´ T ì¶”ë¡ \n",
        "        T = len(self.layers[0].layers)\n",
        "\n",
        "        dhs = np.zeros((N, T, H), dtype='f')\n",
        "        dhs[:, -1, :] = dout\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            dhs = layer.backward(dhs)\n",
        "        return dhs\n",
        "\n",
        "    # í•™ìŠµ/ì¶”ë¡  ëª¨ë“œ ì „í™˜ (Dropout ì œì–´ìš©)\n",
        "    def set_train_flg(self, train_flg=True):\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'train_flg'):\n",
        "                layer.train_flg = train_flg\n",
        "\n",
        "# ==========================================\n",
        "# 4. ë°ì´í„° ë¡œë“œ\n",
        "# ==========================================\n",
        "train_csv = 'split/gillam_train.csv'\n",
        "dev_csv = 'split/gillam_dev.csv'\n",
        "test_csv = 'split/gillam_test.csv'\n",
        "\n",
        "vocab = load_vocab(train_csv, max_vocab=4000)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "x_train, t_train = load_data(train_csv, vocab)\n",
        "x_dev, t_dev = load_data(dev_csv, vocab)\n",
        "x_test, t_test = load_data(test_csv, vocab)\n",
        "\n",
        "if np.__name__ == 'cupy':\n",
        "    x_train, t_train = np.asarray(x_train), np.asarray(t_train)\n",
        "    x_dev, t_dev = np.asarray(x_dev), np.asarray(t_dev)\n",
        "    x_test, t_test = np.asarray(x_test), np.asarray(t_test)\n",
        "\n",
        "print(f\"Train Shape: {x_train.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. í•™ìŠµ (Train Loop ìˆ˜ì •)\n",
        "# ==========================================\n",
        "max_epoch = 15 # Dropoutì„ ì“°ë©´ í•™ìŠµì´ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ Epochì„ ì¡°ê¸ˆ ëŠ˜ë¦¼\n",
        "batch_size = 16\n",
        "hidden_size = 128\n",
        "wordvec_size = 100\n",
        "dropout_ratio = 0.5 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì„¤ì • (ë³´í†µ 0.5)\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = SLPClassifier(vocab_size, wordvec_size, hidden_size, dropout_ratio=dropout_ratio)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "data_size = len(x_train)\n",
        "max_iters = data_size // batch_size\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ (Device: {np.__name__})\")\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    # [ì¤‘ìš”] í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • (Dropout í™œì„±í™”)\n",
        "    model.set_train_flg(True)\n",
        "\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x_train = x_train[idx]\n",
        "    t_train = t_train[idx]\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        batch_x = x_train[i*batch_size : (i+1)*batch_size]\n",
        "        batch_t = t_train[i*batch_size : (i+1)*batch_size]\n",
        "\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # [ì¤‘ìš”] í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë¹„í™œì„±í™”)\n",
        "    model.set_train_flg(False)\n",
        "\n",
        "    # ê²€ì¦ (Dev Set)\n",
        "    model_score = model.predict(x_dev)\n",
        "    pred = np.argmax(model_score, axis=1)\n",
        "    acc = np.sum(pred == t_dev) / len(t_dev)\n",
        "\n",
        "    avg_loss = total_loss / loss_count\n",
        "    print(f\"| Epoch {epoch+1:02} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. ìµœì¢… í…ŒìŠ¤íŠ¸\n",
        "# ==========================================\n",
        "# í…ŒìŠ¤íŠ¸ ì „ì—ë„ í‰ê°€ ëª¨ë“œ í™•ì¸\n",
        "model.set_train_flg(False)\n",
        "\n",
        "test_score = model.predict(x_test)\n",
        "test_pred = np.argmax(test_score, axis=1)\n",
        "test_acc = np.sum(test_pred == t_test) / len(t_test)\n",
        "\n",
        "print(f\"\\nğŸ† Final Test Accuracy: {test_acc*100:.2f}%\")"
      ]
    }
  ]
}