{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSBJu6bu7PU",
        "outputId": "ec0203cc-c3dc-46ff-9e06-3530a7d5b7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ: numpy\n",
            "â³ Vocab ìƒì„± ì¤‘...\n",
            "âœ… Vocab í¬ê¸°: 3002\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_train.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_dev.csv\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”©: split/gillam_test.csv\n",
            "Train Shape: (540, 300), (540,)\n",
            "\n",
            "ğŸš€ í•™ìŠµ ì‹œì‘ (Device: numpy)\n",
            "| Epoch 1 | Loss 0.6069 | Dev Acc 0.7353\n",
            "| Epoch 2 | Loss 0.5235 | Dev Acc 0.7353\n",
            "| Epoch 3 | Loss 0.4686 | Dev Acc 0.7500\n",
            "| Epoch 4 | Loss 0.4011 | Dev Acc 0.7353\n",
            "| Epoch 5 | Loss 0.3520 | Dev Acc 0.7500\n",
            "| Epoch 6 | Loss 0.3164 | Dev Acc 0.7353\n",
            "| Epoch 7 | Loss 0.2875 | Dev Acc 0.7206\n",
            "| Epoch 8 | Loss 0.2948 | Dev Acc 0.7206\n",
            "| Epoch 9 | Loss 0.2595 | Dev Acc 0.7059\n",
            "| Epoch 10 | Loss 0.2722 | Dev Acc 0.7353\n",
            "\n",
            "ğŸ† Final Test Accuracy: 70.59%\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ ì§€ì •\n",
        "# ==========================================\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë³¸ì¸ì˜ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
        "PROJECT_PATH = '/content/drive/MyDrive/DL_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "# ì œê³µëœ common ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "try:\n",
        "    from common.np import * # config.GPUì— ë”°ë¼ numpy ë˜ëŠ” cupyê°€ ë¡œë“œë¨\n",
        "    from common.time_layers import TimeEmbedding, TimeLSTM\n",
        "    from common.layers import Affine, SoftmaxWithLoss\n",
        "    from common.optimizer import Adam\n",
        "    from common.trainer import Trainer\n",
        "    from utils import extract_utterances\n",
        "    import pandas as pd\n",
        "    from collections import Counter\n",
        "    print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ. ì‚¬ìš© ëª¨ë“ˆ:\", np.__name__)\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"í´ë” êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë” (Numpy ìŠ¤íƒ€ì¼)\n",
        "# ==========================================\n",
        "def load_vocab(train_csv, max_vocab=3000):\n",
        "    df = pd.read_csv(train_csv)\n",
        "    words = []\n",
        "    print(\"â³ Vocab ìƒì„± ì¤‘...\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            words.extend(text.split())\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    counter = Counter(words)\n",
        "    most_common = counter.most_common(max_vocab)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for i, (w, _) in enumerate(most_common):\n",
        "        vocab[w] = i + 2\n",
        "\n",
        "    print(f\"âœ… Vocab í¬ê¸°: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "def load_data(csv_path, vocab, max_len=300):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    x_data = []\n",
        "    t_data = []\n",
        "\n",
        "    label_map = {'TD': 0, 'SLI': 1}\n",
        "\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            utts = extract_utterances(row['filename'], ['CHI'])\n",
        "            if not utts: continue\n",
        "\n",
        "            text = \" \".join([u.clean_text for u in utts]).lower()\n",
        "            tokens = text.split()\n",
        "\n",
        "            # ì •ìˆ˜ ì¸ì½”ë”©\n",
        "            ids = [vocab.get(w, vocab['<unk>']) for w in tokens]\n",
        "\n",
        "            # ê¸¸ì´ ì œí•œ\n",
        "            if len(ids) > max_len:\n",
        "                ids = ids[:max_len]\n",
        "            # íŒ¨ë”© (Pre-padding or Post-padding)\n",
        "            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ Post-padding (ë’¤ì— 0 ì±„ì›€)\n",
        "            # ì‹¤ì œ ê¸¸ì´ë¥¼ ê¸°ì–µí–ˆë‹¤ê°€ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜ ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ íŒ¨ë”©ë§Œ ìˆ˜í–‰\n",
        "            padded_ids = np.zeros(max_len, dtype=int)\n",
        "            length = min(len(ids), max_len)\n",
        "            padded_ids[:length] = ids[:length]\n",
        "\n",
        "            x_data.append(padded_ids)\n",
        "            t_data.append(label_map[row['group']])\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Skip: {e}\")\n",
        "            pass\n",
        "\n",
        "    return np.array(x_data), np.array(t_data)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ì •ì˜ (Rnnlm ë³€í˜• - ë¶„ë¥˜ ëª¨ë¸)\n",
        "# ==========================================\n",
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ì •ì˜ (ìˆ˜ì •ë¨)\n",
        "# ==========================================\n",
        "class SLPClassifier:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size, class_size=2):\n",
        "        V, D, H, C = vocab_size, wordvec_size, hidden_size, class_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier)\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, C) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(C).astype('f')\n",
        "\n",
        "        # ê³„ì¸µ ìƒì„±\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False),\n",
        "        ]\n",
        "        self.affine_layer = Affine(affine_W, affine_b)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸° ìˆ˜ì§‘\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        self.params += self.affine_layer.params\n",
        "        self.grads += self.affine_layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "\n",
        "        # xs shape: (N, T, H) -> ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœë§Œ ì‚¬ìš©\n",
        "        h_last = xs[:, -1, :]\n",
        "        score = self.affine_layer.forward(h_last)\n",
        "        return score\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        # 1. Loss -> Affine ì—­ì „íŒŒ\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        dout = self.affine_layer.backward(dout)\n",
        "\n",
        "        # dout shape: (N, H)\n",
        "        N, H = dout.shape\n",
        "\n",
        "        # 2. TimeLSTMìœ¼ë¡œ ë„˜ê²¨ì¤„ ê¸°ìš¸ê¸°(dhs) ìƒì„±\n",
        "        # TimeEmbedding ê³„ì¸µ(self.layers[0])ì´ ë³´ê´€ ì¤‘ì¸ layer ê°œìˆ˜ë¡œ Të¥¼ ì¶”ë¡ \n",
        "        T = len(self.layers[0].layers)\n",
        "\n",
        "        # dhs shape: (N, T, H) ì´ˆê¸°í™”\n",
        "        dhs = np.zeros((N, T, H), dtype='f')\n",
        "\n",
        "        # ë¶„ë¥˜ ë¬¸ì œ(Many-to-One)ì´ë¯€ë¡œ ë§ˆì§€ë§‰ ì‹œì ì—ë§Œ ê¸°ìš¸ê¸° ì£¼ì…\n",
        "        dhs[:, -1, :] = dout\n",
        "\n",
        "        # 3. LSTM -> Embedding ì—­ì „íŒŒ\n",
        "        for layer in reversed(self.layers):\n",
        "            dhs = layer.backward(dhs)\n",
        "\n",
        "        return dhs\n",
        "\n",
        "# ==========================================\n",
        "# 4. ë°ì´í„° ë¡œë“œ ë° ì‹¤í–‰\n",
        "# ==========================================\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "train_csv = 'split/gillam_train.csv'\n",
        "dev_csv = 'split/gillam_dev.csv'\n",
        "test_csv = 'split/gillam_test.csv'\n",
        "\n",
        "# Vocab ìƒì„±\n",
        "vocab = load_vocab(train_csv)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ (Numpy Array)\n",
        "x_train, t_train = load_data(train_csv, vocab)\n",
        "x_dev, t_dev = load_data(dev_csv, vocab)\n",
        "x_test, t_test = load_data(test_csv, vocab)\n",
        "\n",
        "# ë°ì´í„°ë¥¼ CuPy Arrayë¡œ ë³€í™˜ (GPU ëª¨ë“œì¼ ê²½ìš°)\n",
        "if np.__name__ == 'cupy':\n",
        "    x_train, t_train = np.asarray(x_train), np.asarray(t_train)\n",
        "    x_dev, t_dev = np.asarray(x_dev), np.asarray(t_dev)\n",
        "    x_test, t_test = np.asarray(x_test), np.asarray(t_test)\n",
        "\n",
        "print(f\"Train Shape: {x_train.shape}, {t_train.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. í•™ìŠµ (Custom Loop)\n",
        "# ==========================================\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "max_epoch = 10\n",
        "batch_size = 16\n",
        "hidden_size = 128\n",
        "wordvec_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = SLPClassifier(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "data_size = len(x_train)\n",
        "max_iters = data_size // batch_size\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ (Device: {np.__name__})\")\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    # ë°ì´í„° ì…”í”Œ\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x_train = x_train[idx]\n",
        "    t_train = t_train[idx]\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        batch_x = x_train[i*batch_size : (i+1)*batch_size]\n",
        "        batch_t = t_train[i*batch_size : (i+1)*batch_size]\n",
        "\n",
        "        # ê¸°ìš¸ê¸° êµ¬í•˜ê¸°\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "\n",
        "        # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # ì—í­ë§ˆë‹¤ ê²€ì¦ (Accuracy)\n",
        "    model_score = model.predict(x_dev)\n",
        "    pred = np.argmax(model_score, axis=1)\n",
        "    acc = np.sum(pred == t_dev) / len(t_dev)\n",
        "\n",
        "    avg_loss = total_loss / loss_count\n",
        "    print(f\"| Epoch {epoch+1} | Loss {avg_loss:.4f} | Dev Acc {acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. ìµœì¢… í…ŒìŠ¤íŠ¸\n",
        "# ==========================================\n",
        "test_score = model.predict(x_test)\n",
        "test_pred = np.argmax(test_score, axis=1)\n",
        "test_acc = np.sum(test_pred == t_test) / len(t_test)\n",
        "\n",
        "print(f\"\\nğŸ† Final Test Accuracy: {test_acc*100:.2f}%\")"
      ]
    }
  ]
}